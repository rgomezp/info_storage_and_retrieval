{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Homework 1 CSCE 470\n",
    "#2-20-2017\n",
    "#By: Rodrigo Gomez-Palacio\n",
    "\n",
    "\t#*-------------Note---------------*#\n",
    "\t#Jupyter notebook (visual) outputs\n",
    "\t#can be found at\n",
    "\t#url()\n",
    "\t#*--------------------------------*#\n",
    "\n",
    "#Imports and Inputs\n",
    "import nltk\n",
    "#nltk.download()                          #downloads all nltk packages\n",
    "import string\n",
    "import math\n",
    "\n",
    "documents = [\n",
    "    \"Today the aggies won! Go aggies!\", \n",
    "    \"aggies have won today\", \n",
    "    \"the aggies lost last week\", \n",
    "    \"Find the latest Aggies news\", \n",
    "    \"An Aggie is a student at Texas A&M\"\n",
    "]\n",
    "\n",
    "documents2 = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Punctuation removal \n",
    "temp = []\n",
    "for doc in documents:\n",
    "    tokens = doc.translate(None,string.punctuation)\n",
    "    temp.append(tokens)\n",
    "    \n",
    "documents = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "temp = []\n",
    "for doc in documents:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    temp.append(tokens)\n",
    "    \n",
    "documents = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Case Folding\n",
    "for doc in documents:\n",
    "    for term in range(0,len(doc)):\n",
    "        doc[term] = doc[term].lower()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Stopword removal\n",
    "#stopwords from nltk 'corpus' found here: http://www.nltk.org/book/ch02.html\n",
    "stop = set(['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now'])\n",
    "\n",
    "for doc in range(0,len(documents)):\n",
    "    new_doc = []\n",
    "    for term in range(0,len(documents[doc])):\n",
    "        if documents[doc][term] not in stop:\n",
    "        #if term is not in stop set, keep\n",
    "            new_doc.append(documents[doc][term])\n",
    "    documents[doc] = new_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lemmatization using Wordnet Lemmatizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "for doc in documents:\n",
    "    for term in range(0,len(doc)):\n",
    "            doc[term] = lmtzr.lemmatize(doc[term])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Preprocessing Output---\n",
      "['today', 'bought', u'appl', u'delici', 'red']\n",
      "['bought', u'appl', 'today', u'delciou', 'red']\n",
      "['like', u'appl']\n",
      "['yesterday', 'bought', u'pear']\n",
      "[u'clown', 'scare', u'kid', u'haunt', u'hous']\n",
      "['tomorrow', 'buy', u'appl']\n",
      "[u'appl', u'orang', u'pear', 'oh']\n",
      "['today', 'ate', u'appl', 'pear']\n",
      "['yesterday', 'ate', 'pear']\n",
      "[u'haunt', u'hous', u'scare', u'kid', 'clown', u'mask']\n"
     ]
    }
   ],
   "source": [
    "#Stemming with Porter Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for doc in documents:\n",
    "    for term in range(0,len(doc)):\n",
    "            doc[term] = stemmer.stem(doc[term])\n",
    "\n",
    "print \"---Preprocessing Output---\"\n",
    "for i in documents:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Incidence matrix---\n",
      "[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#Build  Term-Document Incidence Matrix\n",
    "\n",
    "#Build set with unique terms\n",
    "terms = []\n",
    "for doc in documents:\n",
    "    for term in doc:\n",
    "        if term not in terms:\n",
    "            terms.append(term)\n",
    "            \n",
    "incidence_matrix = []          #cols: documents,  rows: terms\n",
    "\n",
    "for term in terms:\n",
    "    row = []\n",
    "    for doc in documents:\n",
    "    #if term appears in document: true...else: false\n",
    "        if term in doc:      \n",
    "            row.append(1)\n",
    "        else:\n",
    "            row.append(0)\n",
    "    incidence_matrix.append(row)\n",
    "\n",
    "print \"---Incidence matrix---\"\n",
    "for i in incidence_matrix:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Count_matrix:---\n",
      "[1, 1, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[1, 1, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 0, 0, 1, 1, 1, 0, 0]\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 1, 0, 0, 1, 1, 1, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#Build term-document count matrix (tf_td)\n",
    "\n",
    "count_matrix = []    \n",
    "\n",
    "for term in terms:\n",
    "    row = []\n",
    "    for doc in documents:\n",
    "        count = 0\n",
    "        if term in doc:\n",
    "            for index in range(0,len(doc)):\n",
    "                if doc[index] == term:\n",
    "                    count+=1\n",
    "            row.append(count)\n",
    "        else:\n",
    "            row.append(0)\n",
    "    count_matrix.append(row)\n",
    "print \"---Count_matrix:---\"\n",
    "for i in count_matrix:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc_frequency_vector:\n",
      "[3, 3, 6, 1, 2, 1, 1, 2, 4, 2, 2, 2, 2, 2, 1, 1, 1, 1, 2, 1] \n",
      "\n",
      "\n",
      "Tf_weight_vector:\n",
      "0.52287874528\n",
      "0.52287874528\n",
      "0.221848749616\n",
      "1.0\n",
      "0.698970004336\n",
      "1.0\n",
      "1.0\n",
      "0.698970004336\n",
      "0.397940008672\n",
      "0.698970004336\n",
      "0.698970004336\n",
      "0.698970004336\n",
      "0.698970004336\n",
      "0.698970004336\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.698970004336\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#Build document frequency vector\n",
    "document_freq_vec = []\n",
    "for i in range(0,len(terms)):\n",
    "    count = 0\n",
    "    for doc in documents:\n",
    "        if terms[i] in doc:\n",
    "            count+=1\n",
    "    document_freq_vec.append(count)\n",
    "    \n",
    "print \"Doc_frequency_vector:\\n\", document_freq_vec, \"\\n\\n\"\n",
    "\n",
    "#Build tf-weight vector\n",
    "N = float(len(documents))\n",
    "tf_weight_vec = []\n",
    "for num in document_freq_vec:\n",
    "    weight = math.log10(N/num)\n",
    "    tf_weight_vec.append(weight)\n",
    "    \n",
    "print \"Tf_weight_vector:\"\n",
    "for i in tf_weight_vec:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Tf_idf_matrix---\n",
      "[0.157, 0.157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.157, 0.0, 0.0]\n",
      "[0.157, 0.157, 0.0, 0.157, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.067, 0.067, 0.067, 0.0, 0.0, 0.067, 0.067, 0.067, 0.0, 0.0]\n",
      "[0.301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.21, 0.21, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.301, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0, 0.21, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.12, 0.0, 0.0, 0.12, 0.12, 0.12, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0, 0.21]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0, 0.21]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0, 0.21]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0, 0.21]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.21, 0.0, 0.0, 0.0, 0.0, 0.21]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.301, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.301, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.301, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.301, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21, 0.21, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.301]\n"
     ]
    }
   ],
   "source": [
    "#Build Tf-Idf Matrix\n",
    "tf_idf_matrix = []\n",
    "\n",
    "for term in range(0,len(terms)):\n",
    "    row = []\n",
    "    for doc in range(0,len(documents)):\n",
    "        weight = math.log10(1.0+count_matrix[term][doc])*tf_weight_vec[term]\n",
    "        row.append(round(weight,3))\n",
    "    tf_idf_matrix.append(row)\n",
    "    \n",
    "print \"---Tf_idf_matrix---\" \n",
    "for i in tf_idf_matrix:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transpose tf_idf_matrix\n",
    "tf_idf_matrix_T = zip(*tf_idf_matrix)\n",
    "#print tf_idf_matrix_T\n",
    "\n",
    "norm_tf_idf_matrix_T = []                #normalized tf_idf_matrix transpose\n",
    "#Length Normalization\n",
    "for doc_vec in tf_idf_matrix_T:\n",
    "    #Calculate denominator\n",
    "    sum_of_squares = 0\n",
    "    for num in doc_vec:\n",
    "        sum_of_squares += num**2\n",
    "    denominator = sum_of_squares**(.5)\n",
    "    new_vec = []\n",
    "    for num in doc_vec:\n",
    "        norm_val = num / denominator\n",
    "        new_vec.append(norm_val)\n",
    "    norm_tf_idf_matrix_T.append(new_vec)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Cosine Similarity Matrix---\n",
      "1 [1, 0.519, 0.034, 0.197, 0.0, 0.024, 0.023, 0.227, 0.0, 0.0]\n",
      "2 [0.519, 1, 0.034, 0.197, 0.0, 0.024, 0.023, 0.227, 0.0, 0.0]\n",
      "3 [0.034, 0.034, 1, 0.0, 0.0, 0.034, 0.033, 0.049, 0.0, 0.0]\n",
      "4 [0.197, 0.197, 0.0, 1, 0.0, 0.0, 0.112, 0.169, 0.633, 0.0]\n",
      "5 [0.0, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 0.842]\n",
      "6 [0.024, 0.024, 0.034, 0.0, 0.0, 1, 0.023, 0.035, 0.0, 0.0]\n",
      "7 [0.023, 0.023, 0.033, 0.112, 0.0, 0.023, 1, 0.143, 0.101, 0.0]\n",
      "8 [0.227, 0.227, 0.049, 0.169, 0.0, 0.035, 0.143, 1, 0.617, 0.0]\n",
      "9 [0.0, 0.0, 0.0, 0.633, 0.0, 0.0, 0.101, 0.617, 1, 0.0]\n",
      "10 [0.0, 0.0, 0.0, 0.0, 0.842, 0.0, 0.0, 0.0, 0.0, 1]\n",
      "\n",
      "\n",
      "1 : Today I bought an apple. It was delicious and very red.\n",
      "2 : I bought apples today. They were delcious and very red.\n",
      "3 : I like apples\n",
      "4 : Yesterday I bought some pears\n",
      "5 : Clowns scare kids at the haunted house\n",
      "6 : Tomorrow I will buy some apples\n",
      "7 : Apples, oranges, and pears oh my!\n",
      "8 : Today I ate an apple and a pear\n",
      "9 : Yesterday I ate a pear\n",
      "10 : The haunted house scares kids with clown masks\n"
     ]
    }
   ],
   "source": [
    "#Build 5 x 5 cosine similarity matrix\n",
    "cosine_sim = []\n",
    "\n",
    "for doc_vec1 in range(0,len(norm_tf_idf_matrix_T)):\n",
    "    row = []\n",
    "    for doc_vec2 in range(0,len(norm_tf_idf_matrix_T)):\n",
    "        if doc_vec1 == doc_vec2:\n",
    "            row.append(1)\n",
    "        elif doc_vec1 < doc_vec2:                            #symmetric matrix, don't waste operations on half the mtrx\n",
    "            score = 0\n",
    "            for i in range(0,len(norm_tf_idf_matrix_T[doc_vec1])):\n",
    "                score += (norm_tf_idf_matrix_T[doc_vec1][i]*norm_tf_idf_matrix_T[doc_vec2][i])\n",
    "            row.append(round(score,3))\n",
    "        else:\n",
    "            row.append(0)\n",
    "    cosine_sim.append(row)\n",
    "    \n",
    "#Because symmetric matrix, copy over\n",
    "for doc_vec1 in range(0,len(norm_tf_idf_matrix_T)):\n",
    "    for doc_vec2 in range(0,len(norm_tf_idf_matrix_T)):\n",
    "        if doc_vec1 > doc_vec2:\n",
    "            cosine_sim[doc_vec1][doc_vec2] = cosine_sim[doc_vec2][doc_vec1]\n",
    "\n",
    "print \"---Cosine Similarity Matrix---\"\n",
    "for row in range(0,len(cosine_sim)):\n",
    "    print row+1,cosine_sim[row]\n",
    "print \"\\n\"\n",
    "for doc in range(0,len(documents2)):\n",
    "    print doc+1,\":\", documents2[doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Part 2------------\n",
    "#Linked List Class\n",
    "class Node:\n",
    "    def __init__(self,initdata):\n",
    "        self.data = initdata\n",
    "        self.next = None\n",
    "\n",
    "    def getData(self):\n",
    "        return self.data\n",
    "\n",
    "    def getNext(self):\n",
    "        return self.next\n",
    "\n",
    "    def setData(self,newdata):\n",
    "        self.data = newdata\n",
    "\n",
    "    def setNext(self,newnext):\n",
    "        self.next = newnext\n",
    "        \n",
    "#Term & Frequency Class\n",
    "class Term:\n",
    "    def __init__(self,initterm,initfreq):\n",
    "        self.term = initterm\n",
    "        self.freq = initfreq\n",
    "        \n",
    "#Build Inverted Index\n",
    "head_container = []\n",
    "\n",
    "for row in range(0,len(incidence_matrix)):\n",
    "    count = 0\n",
    "    for i in incidence_matrix[row]:\n",
    "        count += i\n",
    "    term_data = Term(terms[row],count)   #put term and count into struct\n",
    "    node = Node(term_data)               #put struct into node\n",
    "    head_container.append(node)          #put node into head_container\n",
    "    for j in range(0,len(incidence_matrix[row])):\n",
    "        if incidence_matrix[row][j]==1:\n",
    "            temp_node = Node(j)\n",
    "            node.next=temp_node\n",
    "            node = node.next\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ today 3 ] -> 0 -> 1 -> 7 -> //\n",
      "\n",
      "[ bought 3 ] -> 0 -> 1 -> 3 -> //\n",
      "\n",
      "[ appl 6 ] -> 0 -> 1 -> 2 -> 5 -> 6 -> 7 -> //\n",
      "\n",
      "[ delici 1 ] -> 0 -> //\n",
      "\n",
      "[ red 2 ] -> 0 -> 1 -> //\n",
      "\n",
      "[ delciou 1 ] -> 1 -> //\n",
      "\n",
      "[ like 1 ] -> 2 -> //\n",
      "\n",
      "[ yesterday 2 ] -> 3 -> 8 -> //\n",
      "\n",
      "[ pear 4 ] -> 3 -> 6 -> 7 -> 8 -> //\n",
      "\n",
      "[ clown 2 ] -> 4 -> 9 -> //\n",
      "\n",
      "[ scare 2 ] -> 4 -> 9 -> //\n",
      "\n",
      "[ kid 2 ] -> 4 -> 9 -> //\n",
      "\n",
      "[ haunt 2 ] -> 4 -> 9 -> //\n",
      "\n",
      "[ hous 2 ] -> 4 -> 9 -> //\n",
      "\n",
      "[ tomorrow 1 ] -> 5 -> //\n",
      "\n",
      "[ buy 1 ] -> 5 -> //\n",
      "\n",
      "[ orang 1 ] -> 6 -> //\n",
      "\n",
      "[ oh 1 ] -> 6 -> //\n",
      "\n",
      "[ ate 2 ] -> 7 -> 8 -> //\n",
      "\n",
      "[ mask 1 ] -> 9 -> //\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in head_container:\n",
    "    print \"[\", i.data.term, i.data.freq,\"] ->\",\n",
    "    node = i.next\n",
    "    while node is not None:\n",
    "        print node.data,\"->\",\n",
    "        node = node.next\n",
    "    print \"//\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
