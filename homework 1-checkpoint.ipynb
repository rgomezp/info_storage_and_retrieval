{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports and Inputs\n",
    "import nltk\n",
    "#nltk.download()                          #downloads all nltk packages\n",
    "import string\n",
    "import math\n",
    "\n",
    "documents = [\"My name is Rodrigo\", \"To the store, Jim went\", \"Jim went to the store\", \"My parents named me Rodrigo\", \"Cornelius sucks dick\", \"Yesterday I went to the mall to buy some shoes\", \"I bought these shoes at the mall yesterday\",\"Rodrigo bought these shoes\"]\n",
    "#documents = [\"Today the aggies won! Go aggies!\", \"aggies have won today\", \"the aggies lost last week\", \"Find the latest Aggies news\", \"An Aggie is a student at Texas A&M\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Rodrigo\n",
      "To the store Jim went\n",
      "Jim went to the store\n",
      "My parents named me Rodrigo\n",
      "Cornelius sucks dick\n",
      "Yesterday I went to the mall to buy some shoes\n",
      "I bought these shoes at the mall yesterday\n",
      "Rodrigo bought these shoes\n"
     ]
    }
   ],
   "source": [
    "#Punctuation removal \n",
    "temp = []\n",
    "for doc in documents:\n",
    "    tokens = doc.translate(None,string.punctuation)\n",
    "    temp.append(tokens)\n",
    "    \n",
    "documents = temp\n",
    "for i in documents:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Rodrigo']\n",
      "['To', 'the', 'store', 'Jim', 'went']\n",
      "['Jim', 'went', 'to', 'the', 'store']\n",
      "['My', 'parents', 'named', 'me', 'Rodrigo']\n",
      "['Cornelius', 'sucks', 'dick']\n",
      "['Yesterday', 'I', 'went', 'to', 'the', 'mall', 'to', 'buy', 'some', 'shoes']\n",
      "['I', 'bought', 'these', 'shoes', 'at', 'the', 'mall', 'yesterday']\n",
      "['Rodrigo', 'bought', 'these', 'shoes']\n"
     ]
    }
   ],
   "source": [
    "#Tokenize\n",
    "temp = []\n",
    "for doc in documents:\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    temp.append(tokens)\n",
    "    \n",
    "documents = temp\n",
    "for i in documents:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'is', 'rodrigo']\n",
      "['to', 'the', 'store', 'jim', 'went']\n",
      "['jim', 'went', 'to', 'the', 'store']\n",
      "['my', 'parents', 'named', 'me', 'rodrigo']\n",
      "['cornelius', 'sucks', 'dick']\n",
      "['yesterday', 'i', 'went', 'to', 'the', 'mall', 'to', 'buy', 'some', 'shoes']\n",
      "['i', 'bought', 'these', 'shoes', 'at', 'the', 'mall', 'yesterday']\n",
      "['rodrigo', 'bought', 'these', 'shoes']\n"
     ]
    }
   ],
   "source": [
    "#Case Folding\n",
    "for doc in documents:\n",
    "    for term in range(0,len(doc)):\n",
    "        doc[term] = doc[term].lower()\n",
    "        \n",
    "for i in documents:\n",
    "    print i    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'rodrigo']\n",
      "['to', 'store', 'jim', 'went']\n",
      "['jim', 'went', 'to', 'store']\n",
      "['my', 'parents', 'named', 'me', 'rodrigo']\n",
      "['cornelius', 'sucks', 'dick']\n",
      "['yesterday', 'i', 'went', 'to', 'mall', 'to', 'buy', 'some', 'shoes']\n",
      "['i', 'bought', 'these', 'shoes', 'mall', 'yesterday']\n",
      "['rodrigo', 'bought', 'these', 'shoes']\n"
     ]
    }
   ],
   "source": [
    "#Stopword removal\n",
    "from nltk.corpus import stopwords\n",
    "stop = set([\"the\",\"go\",\"have\",\"an\",\"is\",\"a\",\"at\"])\n",
    "\n",
    "for doc in range(0,len(documents)):\n",
    "    new_doc = []\n",
    "    for term in range(0,len(documents[doc])):\n",
    "        if documents[doc][term] not in stop:\n",
    "            new_doc.append(documents[doc][term])\n",
    "    documents[doc] = new_doc\n",
    "for i in documents:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'rodrigo']\n",
      "['to', 'store', 'jim', 'went']\n",
      "['jim', 'went', 'to', 'store']\n",
      "['my', u'parent', 'named', 'me', 'rodrigo']\n",
      "['cornelius', u'suck', 'dick']\n",
      "['yesterday', 'i', 'went', 'to', 'mall', 'to', 'buy', 'some', u'shoe']\n",
      "['i', 'bought', 'these', u'shoe', 'mall', 'yesterday']\n",
      "['rodrigo', 'bought', 'these', u'shoe']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()\n",
    "\n",
    "for doc in documents:\n",
    "    for term in range(0,len(doc)):\n",
    "            doc[term] = lmtzr.lemmatize(doc[term])\n",
    "for i in documents:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed docs:\n",
      "['my', 'name', 'rodrigo']\n",
      "['to', 'store', 'jim', 'went']\n",
      "['jim', 'went', 'to', 'store']\n",
      "['my', u'parent', u'name', 'me', 'rodrigo']\n",
      "[u'corneliu', u'suck', 'dick']\n",
      "['yesterday', 'i', 'went', 'to', 'mall', 'to', 'buy', 'some', u'shoe']\n",
      "['i', 'bought', 'these', u'shoe', 'mall', 'yesterday']\n",
      "['rodrigo', 'bought', 'these', u'shoe']\n"
     ]
    }
   ],
   "source": [
    "#Stemming with Porter Stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for doc in documents:\n",
    "    for term in range(0,len(doc)):\n",
    "            doc[term] = stemmer.stem(doc[term])\n",
    "\n",
    "print \"Stemmed docs:\"\n",
    "for i in documents:\n",
    "    print i\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Incidence matrix:\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[1, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[0, 1, 1, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1]\n",
      "[1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 1, 0, 0, 1, 0, 0]\n",
      "[1, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#Build  term-document incidence matrix\n",
    "\n",
    "#Build set with unique terms\n",
    "terms = set([])\n",
    "for doc in documents:\n",
    "    for term in doc:\n",
    "        if term not in terms:\n",
    "            terms.add(term)\n",
    "            \n",
    "terms = list(terms)\n",
    "\n",
    "incidence_matrix = []          #matrix elements represent terms appearing in documents\n",
    "\n",
    "for term in terms:\n",
    "    row = []\n",
    "    for doc in documents:\n",
    "        if term in doc:\n",
    "            row.append(1)\n",
    "        else:\n",
    "            row.append(0)\n",
    "    incidence_matrix.append(row)\n",
    "\n",
    "print \"\\nIncidence matrix:\"\n",
    "for i in incidence_matrix:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terms:\n",
      "  me\n",
      "  these\n",
      "  corneliu\n",
      "  name\n",
      "  parent\n",
      "  i\n",
      "  dick\n",
      "  jim\n",
      "  some\n",
      "  yesterday\n",
      "  to\n",
      "  mall\n",
      "  buy\n",
      "  shoe\n",
      "  rodrigo\n",
      "  went\n",
      "  my\n",
      "  suck\n",
      "  store\n",
      "  bought\n",
      "\n",
      "\n",
      "Count_matrix:\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[1, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[0, 1, 1, 0, 0, 2, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 0]\n",
      "[0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1]\n",
      "[1, 0, 0, 1, 0, 0, 0, 1]\n",
      "[0, 1, 1, 0, 0, 1, 0, 0]\n",
      "[1, 0, 0, 1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1, 0, 0, 0]\n",
      "[0, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "#Build term-document count matrix (tf_td)\n",
    "\n",
    "count_matrix = []\n",
    "\n",
    "print \"Terms:\"\n",
    "for i in terms:\n",
    "    print \" \", i\n",
    "    \n",
    "print \"\\n\"\n",
    "    \n",
    "\n",
    "for term in terms:\n",
    "    row = []\n",
    "    for doc in documents:\n",
    "        count = 0\n",
    "        if term in doc:\n",
    "            for index in range(0,len(doc)):\n",
    "                if doc[index] == term:\n",
    "                    count+=1\n",
    "            row.append(count)\n",
    "        else:\n",
    "            row.append(0)\n",
    "    count_matrix.append(row)\n",
    "print \"Count_matrix:\"\n",
    "for i in count_matrix:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc_frequency_vector:\n",
      "[1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 3, 2, 1, 3, 3, 3, 2, 1, 2, 2] \n",
      "\n",
      "\n",
      "Tf_weighted_vector:\n",
      "0.903089986992\n",
      "0.602059991328\n",
      "0.903089986992\n",
      "0.602059991328\n",
      "0.903089986992\n",
      "0.602059991328\n",
      "0.903089986992\n",
      "0.602059991328\n",
      "0.903089986992\n",
      "0.602059991328\n",
      "0.301029995664\n",
      "0.602059991328\n",
      "0.903089986992\n",
      "0.301029995664\n",
      "0.301029995664\n",
      "0.301029995664\n",
      "0.602059991328\n",
      "0.903089986992\n",
      "0.602059991328\n",
      "0.602059991328\n"
     ]
    }
   ],
   "source": [
    "#Build document frequency matrix\n",
    "document_freq_vec = []\n",
    "for i in range(0,len(terms)):\n",
    "    count = 0\n",
    "    for doc in documents:\n",
    "        if terms[i] in doc:\n",
    "            count+=1\n",
    "    document_freq_vec.append(count)\n",
    "    \n",
    "print \"Doc_frequency_vector:\\n\", document_freq_vec, \"\\n\\n\"\n",
    "\n",
    "#Build tf-weight vector\n",
    "N = len(documents)\n",
    "tf_weight_vec = []\n",
    "for num in document_freq_vec:\n",
    "    weight = math.log10(N/num)\n",
    "    tf_weight_vec.append(weight)\n",
    "    \n",
    "print \"Tf_weighted_vector:\"\n",
    "for i in tf_weight_vec:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf_idf_matrix:\n",
      "\n",
      "[0.0, 0.0, 0.0, 0.272, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.181, 0.181]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.272, 0.0, 0.0, 0.0]\n",
      "[0.181, 0.0, 0.0, 0.181, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.272, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.181, 0.181, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.272, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.181, 0.181, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.272, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.181, 0.181, 0.0]\n",
      "[0.0, 0.091, 0.091, 0.0, 0.0, 0.144, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.181, 0.181, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.272, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.091, 0.091, 0.091]\n",
      "[0.091, 0.0, 0.0, 0.091, 0.0, 0.0, 0.0, 0.091]\n",
      "[0.0, 0.091, 0.091, 0.0, 0.0, 0.091, 0.0, 0.0]\n",
      "[0.181, 0.0, 0.0, 0.181, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.272, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.181, 0.181, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.181, 0.181]\n"
     ]
    }
   ],
   "source": [
    "#Build Tf-Idf Matrix\n",
    "tf_idf_matrix = []\n",
    "\n",
    "for term in range(0,len(terms)):\n",
    "    row = []\n",
    "    for doc in range(0,len(documents)):\n",
    "        weight = math.log10(1+count_matrix[term][doc])*tf_weight_vec[term]\n",
    "        row.append(round(weight,3))\n",
    "    tf_idf_matrix.append(row)\n",
    "    \n",
    "print \"Tf_idf_matrix:\\n\" \n",
    "for i in tf_idf_matrix:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.6662566656537104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.334968820853523, 0.0, 0.6662566656537104, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6317559181981234, 0.0, 0.0, 0.31762314119353163, 0.0, 0.0, 0.0, 0.0, 0.31762314119353163, 0.0, 0.0, 0.6317559181981234, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.6317559181981234, 0.0, 0.0, 0.31762314119353163, 0.0, 0.0, 0.0, 0.0, 0.31762314119353163, 0.0, 0.0, 0.6317559181981234, 0.0]\n",
      "[0.5775858254283396, 0.0, 0.0, 0.38434939118577005, 0.5775858254283396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.19323643424256948, 0.0, 0.38434939118577005, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.5773502691896257, 0.0, 0.0, 0.0, 0.5773502691896257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5773502691896257, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.33991044874959697, 0.0, 0.0, 0.5108046522645878, 0.33991044874959697, 0.27042599237536996, 0.33991044874959697, 0.5108046522645878, 0.17089420351499074, 0.0, 0.17089420351499074, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.43632068752395575, 0.0, 0.0, 0.0, 0.43632068752395575, 0.0, 0.0, 0.0, 0.43632068752395575, 0.0, 0.43632068752395575, 0.0, 0.21936564952861864, 0.0, 0.0, 0.0, 0.0, 0.0, 0.43632068752395575]\n",
      "[0.0, 0.6317559181981234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.31762314119353163, 0.31762314119353163, 0.0, 0.0, 0.0, 0.0, 0.6317559181981234]\n"
     ]
    }
   ],
   "source": [
    "#Transpose tf_idf_matrix\n",
    "tf_idf_matrix_T = zip(*tf_idf_matrix)\n",
    "#print tf_idf_matrix_T\n",
    "\n",
    "norm_tf_idf_matrix_T = []                #normalized tf_idf_matrix transpose\n",
    "#Length Normalization\n",
    "for doc_vec in tf_idf_matrix_T:\n",
    "    #Calculate denominator\n",
    "    sum_of_squares = 0\n",
    "    for num in doc_vec:\n",
    "        sum_of_squares += num**2\n",
    "    denominator = sum_of_squares**(.5)\n",
    "    new_vec = []\n",
    "    for num in doc_vec:\n",
    "        norm_val = num / denominator\n",
    "        new_vec.append(norm_val)\n",
    "    norm_tf_idf_matrix_T.append(new_vec)\n",
    "    \n",
    "for col in norm_tf_idf_matrix_T:\n",
    "    print col\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0.0, 0.0, 0.5769, 0.0, 0.0, 0.0, 0.1064]\n",
      "[0.0, 1, 1.0, 0.0, 0.0, 0.1402, 0.0, 0.0]\n",
      "[0.0, 1.0, 1, 0.0, 0.0, 0.1402, 0.0, 0.0]\n",
      "[0.5769, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 0.0614]\n",
      "[0.0, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.1402, 0.1402, 0.0, 0.0, 1, 0.4824, 0.0543]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.4824, 1, 0.621]\n",
      "[0.1064, 0.0, 0.0, 0.0614, 0.0, 0.0543, 0.621, 1]\n"
     ]
    }
   ],
   "source": [
    "#Build 5 x 5 cosine similarity matrix\n",
    "cosine_sim = []\n",
    "\n",
    "for doc_vec1 in range(0,len(norm_tf_idf_matrix_T)):\n",
    "    row = []\n",
    "    for doc_vec2 in range(0,len(norm_tf_idf_matrix_T)):\n",
    "        if doc_vec1 == doc_vec2:\n",
    "            row.append(1)\n",
    "        elif doc_vec1 < doc_vec2:                            #symmetric matrix, don't waste operations on half the mtrx\n",
    "            score = 0\n",
    "            for i in range(0,len(norm_tf_idf_matrix_T[doc_vec1])):\n",
    "                score += (norm_tf_idf_matrix_T[doc_vec1][i]*norm_tf_idf_matrix_T[doc_vec2][i])\n",
    "            row.append(round(score,4))\n",
    "        else:\n",
    "            row.append(0)\n",
    "    cosine_sim.append(row)\n",
    "    \n",
    "#Because symmetric matrix, copy over\n",
    "for doc_vec1 in range(0,len(norm_tf_idf_matrix_T)):\n",
    "    for doc_vec2 in range(0,len(norm_tf_idf_matrix_T)):\n",
    "        if doc_vec1 > doc_vec2:\n",
    "            cosine_sim[doc_vec1][doc_vec2] = cosine_sim[doc_vec2][doc_vec1]\n",
    "    \n",
    "for row in cosine_sim:\n",
    "    print row\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'name', 'rodrigo']\n",
      "['to', 'store', 'jim', 'went']\n",
      "['jim', 'went', 'to', 'store']\n",
      "['my', u'parent', u'name', 'me', 'rodrigo']\n",
      "[u'corneliu', u'suck', 'dick']\n",
      "['yesterday', 'i', 'went', 'to', 'mall', 'to', 'buy', 'some', u'shoe']\n",
      "['i', 'bought', 'these', u'shoe', 'mall', 'yesterday']\n",
      "['rodrigo', 'bought', 'these', u'shoe']\n"
     ]
    }
   ],
   "source": [
    "for i in documents:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
